{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class node():\n",
    "    def __init__(self,key,value=None):\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "        self.child = []\n",
    "        \n",
    "    def addChild(self,key,value=None):\n",
    "        self.child.append(node(key,value))\n",
    "        \n",
    "    def insertWord(self,str,value=None):\n",
    "        length = len(str)\n",
    "        if length == 1:\n",
    "            childFound = False\n",
    "            for child in self.child:\n",
    "                if child.key == str:\n",
    "                    child.value = value\n",
    "                    childFound = True\n",
    "            if not childFound:\n",
    "                self.addChild(str,value)\n",
    "        else:\n",
    "            isFound = False\n",
    "            for child in self.child:\n",
    "                if str[0] == child.key:\n",
    "                    isFound = True\n",
    "                    child.insertWord(str[1:],value)\n",
    "            if not isFound:\n",
    "                self.addChild(str[0])\n",
    "                self.child[-1].insertWord(str[1:],value)\n",
    "        \n",
    "    def findMatch(self,string):\n",
    "        length = len(string)\n",
    "        ID = None\n",
    "        Lleft = length\n",
    "        for child in self.child:\n",
    "            if child.key == string[0] and length == 1:\n",
    "                ID = child.value\n",
    "                Lleft = length-1\n",
    "                break\n",
    "            elif child.key == string[0] and length > 1:\n",
    "                ID,Lleft = child.findMatch(string[1:])\n",
    "                break\n",
    "        if ID == None:\n",
    "            ID = self.value\n",
    "            Lleft = length\n",
    "        return ID,Lleft\n",
    "        \n",
    "    def printNode(self):\n",
    "        print(self.key,' ',self.value,end = ':\\n')\n",
    "        for child in self.child:\n",
    "            print(child.key,' ',child.value,end = '  ')\n",
    "        print('\\n-----------------------***-----------------------')\n",
    "        for child in self.child:\n",
    "            child.printNode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabTree():\n",
    "    def __init__(self,vocab):\n",
    "        self.vocab = vocab\n",
    "        self.root = node('*')\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        for i in range(len(self.vocab)):\n",
    "            self.root.insertWord(self.vocab[i],i)\n",
    "        \n",
    "    def findID(self,str):\n",
    "        ID = []\n",
    "        isStart = True\n",
    "        length = len(str)\n",
    "        while(length > 0):\n",
    "            if isStart:\n",
    "                matchId,leftLength = self.root.findMatch(str[-length:])\n",
    "            else:\n",
    "                matchId,leftLength = self.root.findMatch('_'+str[-length:])\n",
    "            ID.append(matchId)\n",
    "            length = leftLength\n",
    "            isStart = False\n",
    "        return ID\n",
    "    \n",
    "    def findWord(self,ID):\n",
    "        return self.vocab[ID]\n",
    "    \n",
    "    def printTree(self):\n",
    "        self.root.printNode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WPM():\n",
    "    def __init__(self,root_dir=None,vocab_source=None,vocab_max_size=1000,file_suffix=r\".trans.txt\"\n",
    "                     ,auto_find_vocab=True,score_threshold=0,fresh_model_mode=False):\n",
    "        if fresh_model_mode:\n",
    "            self.calcScores_ptr = 0\n",
    "            self.vocab_max_size = vocab_max_size\n",
    "            self.vocab_current_size = 0\n",
    "            self.Root_dir = root_dir\n",
    "            self.file_suffix = file_suffix\n",
    "            self.score_threshold = score_threshold\n",
    "            self.vocab = self.BuildStartingVocab()\n",
    "            self.candidateWordPieces = self.BuildStartCandidateWPs()\n",
    "            self.getStartScores()\n",
    "            if auto_find_vocab:\n",
    "                self.GetFullVocab()\n",
    "        else:\n",
    "            self.vocab = self.load(vocab_source)\n",
    "            self.vocab_tree = VocabTree(self.vocab)\n",
    "        \n",
    "    def BuildStartingVocab(self):\n",
    "        vocab = string.ascii_uppercase + \"'\"\n",
    "        vocab = list(vocab)\n",
    "        vocab_extended = []\n",
    "        for subword in vocab:\n",
    "            vocab_extended.append(\"_\"+subword )\n",
    "        vocab += vocab_extended\n",
    "        self.vocab_current_size=len(vocab)\n",
    "        return vocab\n",
    "    \n",
    "    def getIndex(self,char):\n",
    "        if char == \"'\":\n",
    "            char = chr(ord('Z') + 1)\n",
    "        return ord(char )- ord('A')    \n",
    "    \n",
    "    def updateAll(self,update_sample):\n",
    "        for word in update_sample:\n",
    "            middle_word = 0\n",
    "            for i in range(len(word)-1):\n",
    "                index = self.getIndex(word[i])*27 + self.getIndex(word[i+1]) + middle_word*27*27\n",
    "                self.candidateWordPieces[index][1] += 1\n",
    "                middle_word = 1\n",
    "    \n",
    "    def getStartScores(self):\n",
    "        for current_dir,dirs,files in os.walk(self.Root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(self.file_suffix):\n",
    "                    data_sample = self.extractWords(current_dir+os.sep+file)\n",
    "                    self.updateAll(data_sample)\n",
    "        print('Start score is calculated.')\n",
    "        for wordpiece in self.candidateWordPieces:\n",
    "            if wordpiece[1] < self.score_threshold:\n",
    "                del wordpiece\n",
    "        self.calcScores_ptr = len(self.candidateWordPieces)\n",
    "        \n",
    "    \n",
    "    def BuildStartCandidateWPs(self):\n",
    "        outputs = []\n",
    "        for wordpiece1 in self.vocab:\n",
    "            for wordpiece2 in self.vocab:\n",
    "                if not is_start(wordpiece2):\n",
    "                    outputs.append([wordpiece1+wordpiece2[1:],0])\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "    def UpdateCandidateWPs(self):\n",
    "        newWP = self.vocab[-1]\n",
    "        self.calcScores_ptr = len(self.candidateWordPieces)\n",
    "        for  subword in self.vocab:\n",
    "            if not is_start(subword):\n",
    "                self.candidateWordPieces.append([newWP+subword[1:],0])\n",
    "                \n",
    "    def extractWords(self,dir):\n",
    "        file = open(dir)\n",
    "        txt = file.read()\n",
    "        txt = txt.split('\\n')\n",
    "        text = []\n",
    "        for sent in txt:\n",
    "            txt1 = sent.split(' ')\n",
    "            text += [word for word in txt1 if isword(word)]\n",
    "        return text\n",
    "    \n",
    "    def numberOfWordsInFile(str,data_sample):\n",
    "        count = 0\n",
    "        length = len(str)\n",
    "        if is_start(str):\n",
    "            for word in data_sample:\n",
    "                    if word[:length] == str:\n",
    "                        count += 1            \n",
    "        else:\n",
    "            str = str[1:]\n",
    "            for word in data_sample:\n",
    "                if word.find(str,1) > -1:\n",
    "                    count += 1\n",
    "        return count\n",
    "    \n",
    "    def countOccurences(self,str):\n",
    "        numberOfOccurences = 0\n",
    "        for current_dir,dirs,files in os.walk(self.Root_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(self.file_suffix):\n",
    "                    data_sample = extractWords(current_dir+os.sep+file)\n",
    "                    numberOfOccurences += numberOfWordsInFile(str,data_sample)\n",
    "        return numberOfOccurences\n",
    "    \n",
    "    def eliminateCondition(self,wordpiece):\n",
    "        if is_start(wordpiece):\n",
    "            word = '_' + wordpiece[1:]\n",
    "        else:\n",
    "            word = '_' + wordpiece[2:]\n",
    "        for wrd in self.vocab:\n",
    "            if wrd == word:\n",
    "                return False\n",
    "        for wrd in self.candidateWordPieces:\n",
    "            if wrd == word and wrd[1] > self.score_threshold:\n",
    "                return False\n",
    "        return True\n",
    "        \n",
    "    \n",
    "    def calcScores(self):\n",
    "        for wordpiece in self.candidateWordPieces[self.calcScores_ptr:]:\n",
    "            if self.eliminateCondition(wordpiece[0]):\n",
    "                del wordpiece\n",
    "            else:\n",
    "                wordpiece[1] = self.countOccurences(wordpiece[0])\n",
    "                print(wordpiece,' ')\n",
    "                if wordpiece[1] == 0:\n",
    "                    del wordpiece\n",
    "    \n",
    "    def GetFullVocab(self):\n",
    "        while self.vocab_current_size < self.vocab_max_size:\n",
    "            if not self.candidateWordPieces:\n",
    "                break\n",
    "            self.candidateWordPieces.sort(reverse=True,key=lambda x : x[1])\n",
    "            self.vocab.append(self.candidateWordPieces[0][0])\n",
    "            self.vocab_current_size += 1\n",
    "            print('The newly added word ',self.candidateWordPieces[0],' ',self.vocab_current_size,'\\n')\n",
    "            del self.candidateWordPieces[0]\n",
    "            neededWPs = self.vocab_max_size - self.vocab_current_size\n",
    "            if len(self.candidateWordPieces)>neededWPs:\n",
    "                del self.candidateWordPieces[neededWPs:]\n",
    "            self.UpdateCandidateWPs()\n",
    "            self.calcScores()\n",
    "        self.vocab_tree = VocabTree(self.vocab)\n",
    "            \n",
    "    def printVocab(self):\n",
    "        print(self.vocab)\n",
    "        \n",
    "    def getIds(self,vocab):\n",
    "        Id_list = []\n",
    "        for word in vocab :\n",
    "            Id_list += (self.vocab_tree.findID(word))\n",
    "        return Id_list\n",
    "    \n",
    "    def getWords(self,ID_list):\n",
    "        subWord_list=[]\n",
    "        Word_list=[]\n",
    "        for ID in ID_list :\n",
    "            subWord_list.append(self.vocab_tree.findWord(ID))\n",
    "        \n",
    "        return Word_list\n",
    "    \n",
    "    def saveas(self,file):\n",
    "        File = open(file,'a')\n",
    "        for word in self.vocab:\n",
    "            File.write(word+'\\n')\n",
    "        File.close()\n",
    "    \n",
    "    def load(self,file):\n",
    "        File = open(file,'r')\n",
    "        txt = File.read()\n",
    "        vocab = txt.split('\\n')\n",
    "        File.close()\n",
    "        return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(keras.Model):\n",
    "    def __init__(self,sentence_length,vocab_size=1003,language_model_width=512,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embedding = keras.layers.Embedding(vocab_size,100,input_length=sentence_length\n",
    "                                                          ,mask_zero=True)\n",
    "        self.lstm1 = keras.layers.LSTM(language_model_width,return_sequences=True)\n",
    "        self.lstm2 = keras.layers.LSTM(language_model_width,return_sequences=True)\n",
    "        self.lstm3 = keras.layers.LSTM(language_model_width)\n",
    "        self.linear1 = keras.layers.Dense(language_model_width)\n",
    "        self.linear2 = keras.layers.Dense(vocab_size,activation='softmax')\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.lstm1(x) \n",
    "        x = self.lstm2(x) \n",
    "        x = self.lstm3(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessing():\n",
    "    def __init__(self,roots,file_suffix='.trans.txt',max_length=None):\n",
    "        self.text_encoder = WPM(vocab_source=r'C:\\Users\\HH\\Desktop\\project\\newvocab2.txt')\n",
    "        self.file_suffix = file_suffix\n",
    "        self.max_length = max_length\n",
    "        self.roots = roots\n",
    "        if max_length == None:\n",
    "            self.findMaxLength()\n",
    "            \n",
    "        \n",
    "    def findMaxLength(self):\n",
    "        for root in self.roots:\n",
    "            for subdir,dirs,files in os.walk(root):\n",
    "                for file in files:\n",
    "                    if file.endswith(self.file_suffix):\n",
    "                        text = self.getText(subdir+os.sep+file)\n",
    "                        for line in text:\n",
    "                            ids = self.text_encoder.getIds(line)\n",
    "                            line_length = len(line)\n",
    "                            if line_length > self.max_length:\n",
    "                                self.max_length = line_length\n",
    "        \n",
    "    def getText(self,dir):\n",
    "        File = open(dir,'r')\n",
    "        text = File.read()\n",
    "        text = text.split('\\n')\n",
    "        text_samples = []\n",
    "        del text[-1]\n",
    "        for sent in text:\n",
    "            text_samples.append(sent.split(' '))\n",
    "        for sent in text_samples:\n",
    "            sent[0] = '[SOS]'\n",
    "            sent.append('[EOS]')\n",
    "        return text_samples\n",
    "        \n",
    "    def generateBatch(self,root):\n",
    "        IDS = []\n",
    "        x,y = [],[]\n",
    "        for subdir,dirs,files in os.walk(root):\n",
    "            for file in files:\n",
    "                if file.endswith(self.file_suffix):\n",
    "                    text = self.getText(subdir+os.sep+file)\n",
    "                    for sent in text:\n",
    "                        IDS.append(self.text_encoder.getIds(sent))\n",
    "                    for ID_line in IDS:\n",
    "                        for i in range(len(ID_line)-1):\n",
    "                            x.append(ID_line[:i+1])\n",
    "                            y.append(ID_line[i+1])\n",
    "        x = np.array(pad_sequences(x,maxlen=self.max_length,padding='post'))\n",
    "        y = np.array(y)\n",
    "        y = keras.utils.to_categorical(y,num_classes=len(self.text_encoder.vocab))\n",
    "        return x,y\n",
    "    \n",
    "    def Target_Output_Preprocessing(self,max_length,root):\n",
    "        output=[]\n",
    "        for subdir,dirs,files in os.walk(root):\n",
    "                    for file in files:\n",
    "                        if file.endswith(self.file_suffix):\n",
    "                            text = self.getText(subdir+os.sep+file)\n",
    "                            for line in text:\n",
    "                                ids = self.text_encoder.getIds(line)\n",
    "                                output.append(ids)\n",
    "        for line in output:\n",
    "            line.remove(1)\n",
    "        padded_target_output=pad_sequences(output,maxlen=max_length,padding='post')\n",
    "        padded_target_output=keras.utils.to_categorical(padded_target_output,num_classes=977)\n",
    "        padded_target_output=np.array(padded_target_output)\n",
    "        padded_target_output=tf.convert_to_tensor(padded_target_output)\n",
    "        return padded_target_output\n",
    "    \n",
    "    def Decoder_Input_Preprocessing(self,max_length,root):\n",
    "        output=[]\n",
    "        for subdir,dirs,files in os.walk(root):\n",
    "                    for file in files:\n",
    "                        if file.endswith(self.file_suffix):\n",
    "                            text = self.getText(subdir+os.sep+file)\n",
    "                            for line in text:\n",
    "                                ids = self.text_encoder.getIds(line)\n",
    "                                output.append(ids)\n",
    "        padded_decoder_input=pad_sequences(output,maxlen=max_length,padding='post')\n",
    "        padded_decoder_input=np.array(padded_decoder_input)\n",
    "        padded_decoder_input=tf.convert_to_tensor(padded_decoder_input)\n",
    "        return padded_decoder_input\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
